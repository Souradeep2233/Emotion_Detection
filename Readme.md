# Emotion Recognition from Speech ğŸ™ï¸ğŸ˜„ğŸ˜­ğŸ˜ 

This repository contains a Jupyter Notebook (`emotion-recognition.ipynb`) that demonstrates an end-to-end pipeline for recognizing emotions from speech audio using deep learning. The project leverages a hybrid Convolutional Neural Network (CNN) and Transformer architecture to classify audio into various emotional categories.

## ğŸ“š Table of Contents

- [Project Overview](#project-overview)
- [Features](#features)
- [Dataset](#dataset)
- [Model Architecture](#model-architecture)
- [Setup and Installation](#setup-and-installation)
- [Usage](#usage)
- [Emotion Labels](#emotion-labels)
- [Results (Training & Validation)](#results-training--validation)
- [Future Work](#future-work)

## ğŸ“Œ Project Overview

The goal of this project is to develop a robust model that can accurately identify human emotions from spoken words. It involves:

1. ğŸ”Š **Audio Pre-processing:** Converting raw audio into Mel Spectrograms.
2. ğŸ” **Data Augmentation:** Improving model generalization using various audio transformations.
3. ğŸ§  **Model Definition:** Combining CNNs + Transformers for effective learning.
4. ğŸ“ˆ **Training & Evaluation:** Training the model and validating its performance.

## âœ¨ Features

- ğŸµ **Mel Spectrogram Extraction**: Converts waveforms into log-Mel spectrograms.
- ğŸ”„ **Audio Augmentation**: Includes:
  - ğŸ”Š Random volume gain
  - ğŸ”‡ Frequency & time masking
  - ğŸ“ˆ Gaussian noise
  - ğŸšï¸ Pitch shifting
  - ğŸ”ƒ Polarity inversion
  - âœ‚ï¸ Random cropping/padding
- ğŸ§© **Hybrid CNN-Transformer Model (ACNT)**
  - ğŸ§± CNN backbone with residual connections
  - ğŸ•’ Positional encoding for temporal context
  - ğŸ§  Transformer encoder for long-range dependencies
  - ğŸ¯ Classification head for emotion labels
- âš™ï¸ **PyTorch Implementation**
- ğŸš€ **Efficient Data Loading** with custom Dataset & DataLoader

## ğŸ¤ Dataset

Using the RAVDESS dataset:
- Speech & song recordings
- Actors expressing **8 emotions**:
  - ğŸ˜ neutral
  - ğŸ˜Œ calm
  - ğŸ˜Š happy
  - ğŸ˜¢ sad
  - ğŸ˜¡ angry
  - ğŸ˜± fearful
  - ğŸ¤¢ disgust
  - ğŸ˜² surprised

ğŸ—‚ï¸ Dataset Location: `/kaggle/input/ravdess-emotional-speech-audio`

ğŸ”€ Split:
- Train: 70%
- Validation: 15%
- Test: 15% (via 0.5 split of temp 20%)

## ğŸ—ï¸ Model Architecture

### ğŸ” PositionalEncoding
Uses sine and cosine functions to encode sequence order.

### ğŸ”„ ResidualBlock
Improves gradient flow and deep CNN training.

### ğŸ§  ACNT (Audio CNN-Transformer Network)

**Key Parameters:**
- `num_classes`: ğŸ¯ Number of emotion categories
- `in_channel`: ğŸ”ˆ Spectrogram input channel (1 for mono)
- `input_shape`: ğŸ“ Dimensions (e.g., 40x174)
- `cnn_channels`: ğŸ§± [16, 32, 64, 128]
- `transformer_dim`, `num_heads`, `num_layers`: ğŸ§  Transformer specs
- `pe_max_len`, `dropout_rate`, `use_transformer`: âš™ï¸ Flexibility controls

**Architecture Flow:**
1. ğŸ§± CNN layers with BatchNorm, ReLU, Dropout, MaxPool
2. ğŸ”€ Reshape CNN features and project linearly
3. ğŸ•’ Positional encoding (if enabled)
4. ğŸ§  Transformer Encoder for temporal dependencies
5. ğŸ“ Global average pooling
6. ğŸ¯ Classification head

âœ… **Weight Initialization:**
- Kaiming for conv layers
- Xavier for linear layers

## âš™ï¸ Setup and Installation

Install dependencies:
```bash
!pip install torchmetrics
!sudo apt update
!sudo apt install sox libsox-fmt-all
!pip install sox
```

ğŸ“¦ Use Python 3.x. Designed for Kaggle GPU; works with local setups too.

## â–¶ï¸ Usage

1. ğŸ”½ Clone repository:
```bash
git clone <repository_url>
cd <repository_directory>
```
2. ğŸ“¥ Download RAVDESS dataset into `/kaggle/input/ravdess-emotional-speech-audio`
3. ğŸ§ª Run notebook:
```bash
jupyter notebook emotion-recognition.ipynb
```

ğŸƒ The notebook will:
- Load modules
- Extract Mel spectrograms
- Apply augmentations
- Split data
- Initialize ACNT model
- Setup loss, optimizer, scheduler
- Train & validate the model

## ğŸ­ Emotion Labels

```python
Emotion_labels = {
    '01': 'neutral',
    '02': 'calm',
    '03': 'happy',
    '04': 'sad',
    '05': 'angry',
    '06': 'fearful',
    '07': 'disgust',
    '08': 'surprised'
}

emotion_to_idx = {
    'neutral': 0,
    'calm': 1,
    'happy': 2,
    'sad': 3,
    'angry': 4,
    'fearful': 5,
    'disgust': 6,
    'surprised': 7
}
```

## ğŸ“Š Results (Training & Validation)

The training loop prints:
- ğŸ“‰ Training Loss
- ğŸ“‰ Validation Loss
- ğŸ¯ Accuracy on both
- ğŸ” Scheduler adapts learning rate

ğŸ“ˆ MEL spectrogram visualizations included.

## ğŸ”® Future Work

- ğŸ¯ Hyperparameter tuning (grid/Bayesian search)
- ğŸ” K-fold cross-validation
- ğŸ›ï¸ Advanced augmentations (e.g., reverberation)
- ğŸ—ƒï¸ Larger, diverse datasets
- ğŸŒ Model deployment (web/mobile)
- ğŸ§ª Model optimization (quantization/pruning)

---
**Made with â¤ï¸ for Speech Emotion Recognition**
